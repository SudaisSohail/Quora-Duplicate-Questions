{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-21T14:26:52.173682500Z",
     "start_time": "2024-08-21T14:26:32.064819700Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "qs = pd.read_csv(\"org_ques.csv\", usecols=[3, 4, 5]).dropna()\n",
    "qs = pd.concat([qs[qs[\"is_duplicate\"] == 1].sample(100000), qs[qs[\"is_duplicate\"] == 0].sample(100000)])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-21T14:32:23.211516600Z",
     "start_time": "2024-08-21T14:32:19.331991400Z"
    }
   },
   "id": "abe54f634a5f6b22",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                                                question1  \\\n67        Can we ever store energy produced in lightning?   \n70021   Where can I get flexible horse fence solution ...   \n307818              How should I prepare for SBI PO 2017?   \n49510   What are the reasons why many black people hav...   \n130681  What were the major effects of the cambodia ea...   \n...                                                   ...   \n125614  What laptop is best for deep learning experime...   \n329641         Can I ride horses in WorldCraft (Android)?   \n178280                      Which anime are a must watch?   \n403293  What is the highest temperature a human being ...   \n247277  My paytm transaction failed but amount was deb...   \n\n                                                question2  is_duplicate  \n67       Is it possible to store the energy of lightning?             1  \n70021   Where can I found very flexible horse fence sy...             1  \n307818                  How do I prepare for SBI PO 2017?             1  \n49510        Why do some black people have yellow sclera?             1  \n130681  What were the major effects of the cambodia ea...             1  \n...                                                   ...           ...  \n125614  What is the best laptop I can get to learn dee...             0  \n329641    How can you get a Town Hall 9 account for free?             0  \n178280  What are the best anime series to watch? I hav...             0  \n403293  What's the highest temperature someone has had...             0  \n247277         Can I use debit card to pay through Paytm?             0  \n\n[200000 rows x 3 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question1</th>\n      <th>question2</th>\n      <th>is_duplicate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>67</th>\n      <td>Can we ever store energy produced in lightning?</td>\n      <td>Is it possible to store the energy of lightning?</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>70021</th>\n      <td>Where can I get flexible horse fence solution ...</td>\n      <td>Where can I found very flexible horse fence sy...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>307818</th>\n      <td>How should I prepare for SBI PO 2017?</td>\n      <td>How do I prepare for SBI PO 2017?</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>49510</th>\n      <td>What are the reasons why many black people hav...</td>\n      <td>Why do some black people have yellow sclera?</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>130681</th>\n      <td>What were the major effects of the cambodia ea...</td>\n      <td>What were the major effects of the cambodia ea...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>125614</th>\n      <td>What laptop is best for deep learning experime...</td>\n      <td>What is the best laptop I can get to learn dee...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>329641</th>\n      <td>Can I ride horses in WorldCraft (Android)?</td>\n      <td>How can you get a Town Hall 9 account for free?</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>178280</th>\n      <td>Which anime are a must watch?</td>\n      <td>What are the best anime series to watch? I hav...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>403293</th>\n      <td>What is the highest temperature a human being ...</td>\n      <td>What's the highest temperature someone has had...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>247277</th>\n      <td>My paytm transaction failed but amount was deb...</td>\n      <td>Can I use debit card to pay through Paytm?</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>200000 rows × 3 columns</p>\n</div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-21T14:32:23.317110200Z",
     "start_time": "2024-08-21T14:32:23.286316300Z"
    }
   },
   "id": "1ea15e17445e9224",
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "y = qs[\"is_duplicate\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-21T14:32:36.280958600Z",
     "start_time": "2024-08-21T14:32:36.267668Z"
    }
   },
   "id": "ec8411b31ad88eb8",
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "y.to_csv(\"duplicates.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-21T14:32:39.158601500Z",
     "start_time": "2024-08-21T14:32:39.060586300Z"
    }
   },
   "id": "a0161feb64a8f84d",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:164: SyntaxWarning: invalid escape sequence '\\W'\n",
      "<>:164: SyntaxWarning: invalid escape sequence '\\W'\n",
      "C:\\Users\\HC\\AppData\\Local\\Temp\\ipykernel_3088\\2546110395.py:164: SyntaxWarning: invalid escape sequence '\\W'\n",
      "  pattern = re.compile(\"\\W\")\n"
     ]
    }
   ],
   "source": [
    "def preprocess(q):\n",
    "\n",
    "    q = str(q).lower().strip()\n",
    "\n",
    "    # Replace certain special characters with their string equivalents\n",
    "    q = q.replace('%', ' percent')\n",
    "    q = q.replace('$', ' dollar ')\n",
    "    q = q.replace('₹', ' rupee ')\n",
    "    q = q.replace('€', ' euro ')\n",
    "    q = q.replace('@', ' at ')\n",
    "\n",
    "    # The pattern '[math]' appears around 900 times in the whole dataset.\n",
    "    q = q.replace('[math]', '')\n",
    "\n",
    "    # Replacing some numbers with string equivalents (not perfect, can be done better to account for more cases)\n",
    "    q = q.replace(',000,000,000 ', 'b ')\n",
    "    q = q.replace(',000,000 ', 'm ')\n",
    "    q = q.replace(',000 ', 'k ')\n",
    "    q = re.sub(r'([0-9]+)000000000', r'\\1b', q)\n",
    "    q = re.sub(r'([0-9]+)000000', r'\\1m', q)\n",
    "    q = re.sub(r'([0-9]+)000', r'\\1k', q)\n",
    "\n",
    "    contractions = {\n",
    "        \"ain't\": \"am not\",\n",
    "        \"aren't\": \"are not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"can't've\": \"cannot have\",\n",
    "        \"'cause\": \"because\",\n",
    "        \"could've\": \"could have\",\n",
    "        \"couldn't\": \"could not\",\n",
    "        \"couldn't've\": \"could not have\",\n",
    "        \"didn't\": \"did not\",\n",
    "        \"doesn't\": \"does not\",\n",
    "        \"don't\": \"do not\",\n",
    "        \"hadn't\": \"had not\",\n",
    "        \"hadn't've\": \"had not have\",\n",
    "        \"hasn't\": \"has not\",\n",
    "        \"haven't\": \"have not\",\n",
    "        \"he'd\": \"he would\",\n",
    "        \"he'd've\": \"he would have\",\n",
    "        \"he'll\": \"he will\",\n",
    "        \"he'll've\": \"he will have\",\n",
    "        \"he's\": \"he is\",\n",
    "        \"how'd\": \"how did\",\n",
    "        \"how'd'y\": \"how do you\",\n",
    "        \"how'll\": \"how will\",\n",
    "        \"how's\": \"how does\",\n",
    "        \"I'd\": \"I would\",\n",
    "        \"I'd've\": \"I would have\",\n",
    "        \"I'll\": \"I will\",\n",
    "        \"I'll've\": \"I will have\",\n",
    "        \"I'm\": \"I am\",\n",
    "        \"I've\": \"I have\",\n",
    "        \"isn't\": \"is not\",\n",
    "        \"it'd\": \"it would\",\n",
    "        \"it'd've\": \"it would have\",\n",
    "        \"it'll\": \"it will\",\n",
    "        \"it'll've\": \"it will have\",\n",
    "        \"it's\": \"it is\",\n",
    "        \"let's\": \"let us\",\n",
    "        \"ma'am\": \"madam\",\n",
    "        \"mayn't\": \"may not\",\n",
    "        \"might've\": \"might have\",\n",
    "        \"mightn't\": \"might not\",\n",
    "        \"mightn't've\": \"might not have\",\n",
    "        \"must've\": \"must have\",\n",
    "        \"mustn't\": \"must not\",\n",
    "        \"mustn't've\": \"must not have\",\n",
    "        \"needn't\": \"need not\",\n",
    "        \"needn't've\": \"need not have\",\n",
    "        \"o'clock\": \"of the clock\",\n",
    "        \"oughtn't\": \"ought not\",\n",
    "        \"oughtn't've\": \"ought not have\",\n",
    "        \"shan't\": \"shall not\",\n",
    "        \"sha'n't\": \"shall not\",\n",
    "        \"shan't've\": \"shall not have\",\n",
    "        \"she'd\": \"she would\",\n",
    "        \"she'd've\": \"she would have\",\n",
    "        \"she'll\": \"she will\",\n",
    "        \"she'll've\": \"she will have\",\n",
    "        \"she's\": \"she is\",\n",
    "        \"should've\": \"should have\",\n",
    "        \"shouldn't\": \"should not\",\n",
    "        \"shouldn't've\": \"should not have\",\n",
    "        \"so've\": \"so have\",\n",
    "        \"so's\": \"so is\",\n",
    "        \"that'd\": \"that had\",\n",
    "        \"that'd've\": \"that would have\",\n",
    "        \"that's\": \"that is\",\n",
    "        \"there'd\": \"there would\",\n",
    "        \"there'd've\": \"there would have\",\n",
    "        \"there's\": \"there is\",\n",
    "        \"they'd\": \"they would\",\n",
    "        \"they'd've\": \"they would have\",\n",
    "        \"they'll\": \"they will\",\n",
    "        \"they'll've\": \"they will have\",\n",
    "        \"they're\": \"they are\",\n",
    "        \"they've\": \"they have\",\n",
    "        \"to've\": \"to have\",\n",
    "        \"wasn't\": \"was not\",\n",
    "        \"we'd\": \"we would\",\n",
    "        \"we'd've\": \"we would have\",\n",
    "        \"we'll\": \"we will\",\n",
    "        \"we'll've\": \"we will have\",\n",
    "        \"we're\": \"we are\",\n",
    "        \"we've\": \"we have\",\n",
    "        \"weren't\": \"were not\",\n",
    "        \"what'll\": \"what will\",\n",
    "        \"what'll've\": \"what will have\",\n",
    "        \"what're\": \"what are\",\n",
    "        \"what's\": \"what is\",\n",
    "        \"what've\": \"what have\",\n",
    "        \"when's\": \"when is\",\n",
    "        \"when've\": \"when have\",\n",
    "        \"where'd\": \"where did\",\n",
    "        \"where's\": \"where is\",\n",
    "        \"where've\": \"where have\",\n",
    "        \"who'll\": \"who will\",\n",
    "        \"who'll've\": \"who will have\",\n",
    "        \"who's\": \"who is\",\n",
    "        \"who've\": \"who have\",\n",
    "        \"why's\": \"why is\",\n",
    "        \"why've\": \"why have\",\n",
    "        \"will've\": \"will have\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"won't've\": \"will not have\",\n",
    "        \"would've\": \"would have\",\n",
    "        \"wouldn't\": \"would not\",\n",
    "        \"wouldn't've\": \"would not have\",\n",
    "        \"y'all\": \"you all\",\n",
    "        \"y'all'd\": \"you all would\",\n",
    "        \"y'all'd've\": \"you all would have\",\n",
    "        \"y'all're\": \"you all are\",\n",
    "        \"y'all've\": \"you all have\",\n",
    "        \"you'd\": \"you would\",\n",
    "        \"you'd've\": \"you would have\",\n",
    "        \"you'll\": \"you will\",\n",
    "        \"you'll've\": \"you will have\",\n",
    "        \"you're\": \"you are\",\n",
    "        \"you've\": \"you have\"\n",
    "    }\n",
    "\n",
    "    q_decontracted = []\n",
    "\n",
    "    for word in q.split():\n",
    "        if word in contractions:\n",
    "            word = contractions[word]\n",
    "    \n",
    "        q_decontracted.append(word)\n",
    "    \n",
    "    q = ' '.join(q_decontracted)\n",
    "    q = q.replace(\"'ve\", \" have\")\n",
    "    q = q.replace(\"n't\", \" not\")\n",
    "    q = q.replace(\"'re\", \" are\")\n",
    "    q = q.replace(\"'ll\", \" will\")\n",
    "    \n",
    "    # Removing HTML tags\n",
    "    # q = BeautifulSoup(q)\n",
    "    # q = q.get_text()\n",
    "    # \n",
    "    # q = textblob.TextBlob(q).correct().string\n",
    "    \n",
    "    # Remove punctuations\n",
    "    pattern = re.compile(\"\\W\")\n",
    "    q = re.sub(pattern, ' ', q).strip()\n",
    "    \n",
    "    return q"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-21T14:32:50.680171200Z",
     "start_time": "2024-08-21T14:32:50.655636100Z"
    }
   },
   "id": "77b3183b750cb493",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "qs[\"question1\"] = qs[\"question1\"].apply(preprocess)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-21T14:33:05.970509700Z",
     "start_time": "2024-08-21T14:32:56.441892600Z"
    }
   },
   "id": "b358fb9364acb2f1",
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "qs[\"question2\"] = qs[\"question2\"].apply(preprocess)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-21T14:33:11.659523900Z",
     "start_time": "2024-08-21T14:33:05.973502200Z"
    }
   },
   "id": "170ff9cf22781c31",
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "qs['q1_len'] = qs['question1'].str.len()\n",
    "qs['q2_len'] = qs['question2'].str.len()\n",
    "qs['q1_num_words'] = qs['question1'].apply(lambda row: len(row.split(\" \")))\n",
    "qs['q2_num_words'] = qs['question2'].apply(lambda row: len(row.split(\" \")))\n",
    "\n",
    "def common_words(row):\n",
    "    w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n",
    "    w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))\n",
    "    return len(w1 & w2)\n",
    "\n",
    "qs['word_common'] = qs.apply(common_words, axis=1)\n",
    "\n",
    "def total_words(row):\n",
    "    w1 = set(map(lambda word: word.lower().strip(), row['question1'].split(\" \")))\n",
    "    w2 = set(map(lambda word: word.lower().strip(), row['question2'].split(\" \")))\n",
    "    return len(w1) + len(w2)\n",
    "\n",
    "qs['word_total'] = qs.apply(total_words, axis=1)\n",
    "qs['word_share'] = round(qs['word_common']/qs['word_total'],2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-21T14:33:36.224831800Z",
     "start_time": "2024-08-21T14:33:26.694427700Z"
    }
   },
   "id": "58a3b71047c3eca7",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def fetch_token_features(row):\n",
    "\n",
    "    q1 = row['question1']\n",
    "    q2 = row['question2']\n",
    "\n",
    "    SAFE_DIV = 0.0001\n",
    "\n",
    "    STOP_WORDS = stopwords.words(\"english\")\n",
    "\n",
    "    token_features = [0.0]*8\n",
    "\n",
    "    # Converting the Sentence into Tokens: \n",
    "    q1_tokens = q1.split()\n",
    "    q2_tokens = q2.split()\n",
    "\n",
    "    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n",
    "        return token_features\n",
    "\n",
    "    # Get the non-stopwords in Questions\n",
    "    q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])\n",
    "    q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])\n",
    "\n",
    "    #Get the stopwords in Questions\n",
    "    q1_stops = set([word for word in q1_tokens if word in STOP_WORDS])\n",
    "    q2_stops = set([word for word in q2_tokens if word in STOP_WORDS])\n",
    "\n",
    "    # Get the common non-stopwords from Question pair\n",
    "    common_word_count = len(q1_words.intersection(q2_words))\n",
    "\n",
    "    # Get the common stopwords from Question pair\n",
    "    common_stop_count = len(q1_stops.intersection(q2_stops))\n",
    "\n",
    "    # Get the common Tokens from Question pair\n",
    "    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n",
    "\n",
    "\n",
    "    token_features[0] = common_word_count / (min(len(q1_words), len(q2_words)) + SAFE_DIV)\n",
    "    token_features[1] = common_word_count / (max(len(q1_words), len(q2_words)) + SAFE_DIV)\n",
    "    token_features[2] = common_stop_count / (min(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n",
    "    token_features[3] = common_stop_count / (max(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n",
    "    token_features[4] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n",
    "    token_features[5] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n",
    "\n",
    "    # Last word of both question is same or not\n",
    "    token_features[6] = int(q1_tokens[-1] == q2_tokens[-1])\n",
    "\n",
    "    # First word of both question is same or not\n",
    "    token_features[7] = int(q1_tokens[0] == q2_tokens[0])\n",
    "\n",
    "    return token_features\n",
    "token_features = qs.apply(fetch_token_features, axis=1)\n",
    "\n",
    "qs[\"cwc_min\"]       = list(map(lambda x: x[0], token_features))\n",
    "qs[\"cwc_max\"]       = list(map(lambda x: x[1], token_features))\n",
    "qs[\"csc_min\"]       = list(map(lambda x: x[2], token_features))\n",
    "qs[\"csc_max\"]       = list(map(lambda x: x[3], token_features))\n",
    "qs[\"ctc_min\"]       = list(map(lambda x: x[4], token_features))\n",
    "qs[\"ctc_max\"]       = list(map(lambda x: x[5], token_features))\n",
    "qs[\"last_word_eq\"]  = list(map(lambda x: x[6], token_features))\n",
    "qs[\"first_word_eq\"] = list(map(lambda x: x[7], token_features))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-21T14:35:06.015675300Z",
     "start_time": "2024-08-21T14:33:38.401159500Z"
    }
   },
   "id": "efd5f2449a896682",
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import distance\n",
    "\n",
    "def fetch_length_features(row):\n",
    "\n",
    "    q1 = row['question1']\n",
    "    q2 = row['question2']\n",
    "\n",
    "    length_features = [0.0]*3\n",
    "\n",
    "    # Converting the Sentence into Tokens: \n",
    "    q1_tokens = q1.split()\n",
    "    q2_tokens = q2.split()\n",
    "\n",
    "    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n",
    "        return length_features\n",
    "\n",
    "    # Absolute length features\n",
    "    length_features[0] = abs(len(q1_tokens) - len(q2_tokens))\n",
    "\n",
    "    #Average Token Length of both Questions\n",
    "    length_features[1] = (len(q1_tokens) + len(q2_tokens))/2\n",
    "\n",
    "    strs = list(distance.lcsubstrings(q1, q2))\n",
    "    if len(strs) > 0:\n",
    "        length_features[2] = len(strs[0]) / (min(len(q1), len(q2)) + 1)\n",
    "    else:\n",
    "        length_features[2] = 0\n",
    "\n",
    "    return length_features\n",
    "\n",
    "length_features = qs.apply(fetch_length_features, axis=1)\n",
    "\n",
    "qs['abs_len_diff'] = list(map(lambda x: x[0], length_features))\n",
    "qs['mean_len'] = list(map(lambda x: x[1], length_features))\n",
    "qs['longest_substr_ratio'] = list(map(lambda x: x[2], length_features))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-21T14:38:09.714489200Z",
     "start_time": "2024-08-21T14:35:41.369170200Z"
    }
   },
   "id": "14dbe5cd385b0228",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HC\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def fetch_fuzzy_features(row):\n",
    "\n",
    "    q1 = row['question1']\n",
    "    q2 = row['question2']\n",
    "\n",
    "    fuzzy_features = [0.0]*4\n",
    "\n",
    "    # fuzz_ratio\n",
    "    fuzzy_features[0] = fuzz.QRatio(q1, q2)\n",
    "\n",
    "    # fuzz_partial_ratio\n",
    "    fuzzy_features[1] = fuzz.partial_ratio(q1, q2)\n",
    "\n",
    "    # token_sort_ratio\n",
    "    fuzzy_features[2] = fuzz.token_sort_ratio(q1, q2)\n",
    "\n",
    "    # token_set_ratio\n",
    "    fuzzy_features[3] = fuzz.token_set_ratio(q1, q2)\n",
    "\n",
    "    return fuzzy_features\n",
    "fuzzy_features = qs.apply(fetch_fuzzy_features, axis=1)\n",
    "\n",
    "# Creating new feature columns for fuzzy features\n",
    "qs['fuzz_ratio'] = list(map(lambda x: x[0], fuzzy_features))\n",
    "qs['fuzz_partial_ratio'] = list(map(lambda x: x[1], fuzzy_features))\n",
    "qs['token_sort_ratio'] = list(map(lambda x: x[2], fuzzy_features))\n",
    "qs['token_set_ratio'] = list(map(lambda x: x[3], fuzzy_features))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-21T14:45:55.727083500Z",
     "start_time": "2024-08-21T14:38:09.718477600Z"
    }
   },
   "id": "ff2d5a3a34246bfc",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                                                question1  \\\n67         can we ever store energy produced in lightning   \n70021   where can i get flexible horse fence solution ...   \n307818               how should i prepare for sbi po 2017   \n49510   what are the reasons why many black people hav...   \n130681  what were the major effects of the cambodia ea...   \n...                                                   ...   \n125614  what laptop is best for deep learning experiments   \n329641           can i ride horses in worldcraft  android   \n178280                       which anime are a must watch   \n403293  what is the highest temperature a human being ...   \n247277  my paytm transaction failed but amount was deb...   \n\n                                                question2  is_duplicate  \\\n67        is it possible to store the energy of lightning             1   \n70021   where can i found very flexible horse fence sy...             1   \n307818                   how do i prepare for sbi po 2017             1   \n49510         why do some black people have yellow sclera             1   \n130681  what were the major effects of the cambodia ea...             1   \n...                                                   ...           ...   \n125614  what is the best laptop i can get to learn dee...             0   \n329641     how can you get a town hall 9 account for free             0   \n178280  what are the best anime series to watch  i hav...             0   \n403293  what is the highest temperature someone has ha...             0   \n247277          can i use debit card to pay through paytm             0   \n\n        q1_len  q2_len  q1_num_words  q2_num_words  word_common  word_total  \\\n67          46      47             8             9            3          17   \n70021       55      68            10            12            7          22   \n307818      36      32             8             8            7          16   \n49510       59      43            11             8            5          19   \n130681     124     120            22            22           16          36   \n...        ...     ...           ...           ...          ...         ...   \n125614      49      66             8            14            6          22   \n329641      40      46             8            11            1          19   \n178280      28      78             6            17            3          21   \n403293      57      57            10            10            5          20   \n247277      77      41            15             9            2          24   \n\n        word_share  ...   ctc_max  last_word_eq  first_word_eq  abs_len_diff  \\\n67            0.18  ...  0.333330           1.0            0.0           1.0   \n70021         0.32  ...  0.583328           1.0            1.0           2.0   \n307818        0.44  ...  0.874989           1.0            1.0           0.0   \n49510         0.26  ...  0.454541           0.0            0.0           3.0   \n130681        0.44  ...  0.714282           0.0            1.0           0.0   \n...            ...  ...       ...           ...            ...           ...   \n125614        0.27  ...  0.428568           0.0            1.0           6.0   \n329641        0.05  ...  0.090908           0.0            0.0           4.0   \n178280        0.14  ...  0.187499           0.0            0.0          10.0   \n403293        0.25  ...  0.499995           0.0            1.0           0.0   \n247277        0.08  ...  0.142856           0.0            0.0           5.0   \n\n        mean_len  longest_substr_ratio  fuzz_ratio  fuzz_partial_ratio  \\\n67           8.5              0.212766          60                  65   \n70021       11.0              0.410714          73                  67   \n307818       8.0              0.787879          91                  92   \n49510        9.5              0.590909          63                  79   \n130681      21.0              0.793388          93                  93   \n...          ...                   ...         ...                 ...   \n125614      11.0              0.300000          63                  67   \n329641       9.0              0.097561          26                  30   \n178280      11.0              0.241379          38                  61   \n403293      10.0              0.551724          70                  70   \n247277      11.5              0.142857          12                  41   \n\n        token_sort_ratio  token_set_ratio  \n67                    67               67  \n70021                 65               83  \n307818                85               95  \n49510                 69               79  \n130681                92               94  \n...                  ...              ...  \n125614                63               80  \n329641                31               33  \n178280                46               70  \n403293                67               74  \n247277                41               44  \n\n[200000 rows x 25 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question1</th>\n      <th>question2</th>\n      <th>is_duplicate</th>\n      <th>q1_len</th>\n      <th>q2_len</th>\n      <th>q1_num_words</th>\n      <th>q2_num_words</th>\n      <th>word_common</th>\n      <th>word_total</th>\n      <th>word_share</th>\n      <th>...</th>\n      <th>ctc_max</th>\n      <th>last_word_eq</th>\n      <th>first_word_eq</th>\n      <th>abs_len_diff</th>\n      <th>mean_len</th>\n      <th>longest_substr_ratio</th>\n      <th>fuzz_ratio</th>\n      <th>fuzz_partial_ratio</th>\n      <th>token_sort_ratio</th>\n      <th>token_set_ratio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>67</th>\n      <td>can we ever store energy produced in lightning</td>\n      <td>is it possible to store the energy of lightning</td>\n      <td>1</td>\n      <td>46</td>\n      <td>47</td>\n      <td>8</td>\n      <td>9</td>\n      <td>3</td>\n      <td>17</td>\n      <td>0.18</td>\n      <td>...</td>\n      <td>0.333330</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>8.5</td>\n      <td>0.212766</td>\n      <td>60</td>\n      <td>65</td>\n      <td>67</td>\n      <td>67</td>\n    </tr>\n    <tr>\n      <th>70021</th>\n      <td>where can i get flexible horse fence solution ...</td>\n      <td>where can i found very flexible horse fence sy...</td>\n      <td>1</td>\n      <td>55</td>\n      <td>68</td>\n      <td>10</td>\n      <td>12</td>\n      <td>7</td>\n      <td>22</td>\n      <td>0.32</td>\n      <td>...</td>\n      <td>0.583328</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>2.0</td>\n      <td>11.0</td>\n      <td>0.410714</td>\n      <td>73</td>\n      <td>67</td>\n      <td>65</td>\n      <td>83</td>\n    </tr>\n    <tr>\n      <th>307818</th>\n      <td>how should i prepare for sbi po 2017</td>\n      <td>how do i prepare for sbi po 2017</td>\n      <td>1</td>\n      <td>36</td>\n      <td>32</td>\n      <td>8</td>\n      <td>8</td>\n      <td>7</td>\n      <td>16</td>\n      <td>0.44</td>\n      <td>...</td>\n      <td>0.874989</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>8.0</td>\n      <td>0.787879</td>\n      <td>91</td>\n      <td>92</td>\n      <td>85</td>\n      <td>95</td>\n    </tr>\n    <tr>\n      <th>49510</th>\n      <td>what are the reasons why many black people hav...</td>\n      <td>why do some black people have yellow sclera</td>\n      <td>1</td>\n      <td>59</td>\n      <td>43</td>\n      <td>11</td>\n      <td>8</td>\n      <td>5</td>\n      <td>19</td>\n      <td>0.26</td>\n      <td>...</td>\n      <td>0.454541</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>9.5</td>\n      <td>0.590909</td>\n      <td>63</td>\n      <td>79</td>\n      <td>69</td>\n      <td>79</td>\n    </tr>\n    <tr>\n      <th>130681</th>\n      <td>what were the major effects of the cambodia ea...</td>\n      <td>what were the major effects of the cambodia ea...</td>\n      <td>1</td>\n      <td>124</td>\n      <td>120</td>\n      <td>22</td>\n      <td>22</td>\n      <td>16</td>\n      <td>36</td>\n      <td>0.44</td>\n      <td>...</td>\n      <td>0.714282</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>21.0</td>\n      <td>0.793388</td>\n      <td>93</td>\n      <td>93</td>\n      <td>92</td>\n      <td>94</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>125614</th>\n      <td>what laptop is best for deep learning experiments</td>\n      <td>what is the best laptop i can get to learn dee...</td>\n      <td>0</td>\n      <td>49</td>\n      <td>66</td>\n      <td>8</td>\n      <td>14</td>\n      <td>6</td>\n      <td>22</td>\n      <td>0.27</td>\n      <td>...</td>\n      <td>0.428568</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>6.0</td>\n      <td>11.0</td>\n      <td>0.300000</td>\n      <td>63</td>\n      <td>67</td>\n      <td>63</td>\n      <td>80</td>\n    </tr>\n    <tr>\n      <th>329641</th>\n      <td>can i ride horses in worldcraft  android</td>\n      <td>how can you get a town hall 9 account for free</td>\n      <td>0</td>\n      <td>40</td>\n      <td>46</td>\n      <td>8</td>\n      <td>11</td>\n      <td>1</td>\n      <td>19</td>\n      <td>0.05</td>\n      <td>...</td>\n      <td>0.090908</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>4.0</td>\n      <td>9.0</td>\n      <td>0.097561</td>\n      <td>26</td>\n      <td>30</td>\n      <td>31</td>\n      <td>33</td>\n    </tr>\n    <tr>\n      <th>178280</th>\n      <td>which anime are a must watch</td>\n      <td>what are the best anime series to watch  i hav...</td>\n      <td>0</td>\n      <td>28</td>\n      <td>78</td>\n      <td>6</td>\n      <td>17</td>\n      <td>3</td>\n      <td>21</td>\n      <td>0.14</td>\n      <td>...</td>\n      <td>0.187499</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>10.0</td>\n      <td>11.0</td>\n      <td>0.241379</td>\n      <td>38</td>\n      <td>61</td>\n      <td>46</td>\n      <td>70</td>\n    </tr>\n    <tr>\n      <th>403293</th>\n      <td>what is the highest temperature a human being ...</td>\n      <td>what is the highest temperature someone has ha...</td>\n      <td>0</td>\n      <td>57</td>\n      <td>57</td>\n      <td>10</td>\n      <td>10</td>\n      <td>5</td>\n      <td>20</td>\n      <td>0.25</td>\n      <td>...</td>\n      <td>0.499995</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>10.0</td>\n      <td>0.551724</td>\n      <td>70</td>\n      <td>70</td>\n      <td>67</td>\n      <td>74</td>\n    </tr>\n    <tr>\n      <th>247277</th>\n      <td>my paytm transaction failed but amount was deb...</td>\n      <td>can i use debit card to pay through paytm</td>\n      <td>0</td>\n      <td>77</td>\n      <td>41</td>\n      <td>15</td>\n      <td>9</td>\n      <td>2</td>\n      <td>24</td>\n      <td>0.08</td>\n      <td>...</td>\n      <td>0.142856</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5.0</td>\n      <td>11.5</td>\n      <td>0.142857</td>\n      <td>12</td>\n      <td>41</td>\n      <td>41</td>\n      <td>44</td>\n    </tr>\n  </tbody>\n</table>\n<p>200000 rows × 25 columns</p>\n</div>"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-21T15:05:28.891435300Z",
     "start_time": "2024-08-21T15:05:28.605453700Z"
    }
   },
   "id": "c70c73865dad8c58",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "qs.to_csv('ques_with_heuristic_feat.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-21T15:05:43.469613200Z",
     "start_time": "2024-08-21T15:05:39.474540400Z"
    }
   },
   "id": "b3ae586dd8ca1ad4",
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
